{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 08:41:05,606 - INFO - Starting FAQ scraping process\n",
      "2025-03-10 08:41:05,607 - INFO - Loaded 2 websites from 'websites.json'\n",
      "2025-03-10 08:41:05,607 - INFO - Attempting to scrape https://www.irishimmigration.ie/registering-your-immigration-permission/frequently-asked-questions-for-registration/ with accordion ID 'accordion-17799-1'\n",
      "2025-03-10 08:41:06,950 - INFO - Found 39 FAQs on https://www.irishimmigration.ie/registering-your-immigration-permission/frequently-asked-questions-for-registration/\n",
      "2025-03-10 08:41:06,951 - INFO - Attempting to scrape https://www.irishimmigration.ie/coming-to-visit-ireland/frequently-asked-questions/ with accordion ID 'accordion-14617-1'\n",
      "2025-03-10 08:41:07,167 - INFO - Found 17 FAQs on https://www.irishimmigration.ie/coming-to-visit-ireland/frequently-asked-questions/\n",
      "2025-03-10 08:41:07,169 - INFO - Combined FAQs saved to 'combined_faqs.csv'. Total FAQs: 56\n",
      "2025-03-10 08:41:07,169 - INFO - Scraping process completed\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"scrape_log.log\", encoding='utf-8'),\n",
    "        logging.StreamHandler()  # Also prints to console\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def scrape_faq_from_website(url, accordion_id):\n",
    "    \"\"\"Scrape FAQs from a single website given its URL and accordion ID.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Attempting to scrape {url} with accordion ID '{accordion_id}'\")\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            return []\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        accordion = soup.find(id=accordion_id)\n",
    "        \n",
    "        if not accordion:\n",
    "            logger.warning(f\"Accordion with ID '{accordion_id}' not found on {url}\")\n",
    "            return []\n",
    "        \n",
    "        faq_divs = accordion.find_all('div', recursive=False)\n",
    "        if not faq_divs:\n",
    "            logger.warning(f\"No FAQ divs found in accordion '{accordion_id}' on {url}\")\n",
    "            return []\n",
    "        \n",
    "        faq_data = []\n",
    "        for div in faq_divs:\n",
    "            # Find the question\n",
    "            question_elem = div.find('a')\n",
    "            if question_elem and question_elem.find('span'):\n",
    "                question = question_elem.find_all('span')[-1].get_text(strip=True)\n",
    "            else:\n",
    "                question = \"Question not found\"\n",
    "                logger.debug(f\"Question not found in a div on {url}\")\n",
    "                \n",
    "            # Find the answer\n",
    "            answer_div = div.find('div', id=lambda x: x and x != question_elem.get('id') if question_elem else None)\n",
    "            answer_text = \"\"\n",
    "                \n",
    "            if answer_div:\n",
    "                for elem in answer_div.children:\n",
    "                    if elem.name == 'p':\n",
    "                        answer_text += elem.get_text(strip=True) + \"\\n\"\n",
    "                    elif elem.name == 'ul':\n",
    "                        for li in elem.find_all('li'):\n",
    "                            answer_text += f\"- {li.get_text(strip=True)}\\n\"\n",
    "                    elif elem.name == 'section':\n",
    "                        answer_text += elem.get_text(strip=True) + \"\\n\"\n",
    "                    elif elem.name == 'div':\n",
    "                        answer_text += elem.get_text(strip=True) + \"\\n\"\n",
    "                answer_text = answer_text.strip()\n",
    "            else:\n",
    "                answer_text = \"Answer not found\"\n",
    "                logger.debug(f\"Answer not found for a question on {url}\")\n",
    "                \n",
    "            if question != \"Question not found\" or answer_text != \"Answer not found\":\n",
    "                faq_data.append({\n",
    "                    \"Website\": url,\n",
    "                    \"Question\": question,\n",
    "                    \"Answer\": answer_text\n",
    "                })\n",
    "        \n",
    "        logger.info(f\"Found {len(faq_data)} FAQs on {url}\")\n",
    "        return faq_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping {url}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def get_websites_from_user():\n",
    "    \"\"\"Prompt user to input websites and accordion IDs interactively.\"\"\"\n",
    "    websites = []\n",
    "    logger.info(\"Starting interactive website input\")\n",
    "    print(\"Enter websites and their accordion IDs. Type 'done' when finished.\")\n",
    "    while True:\n",
    "        url = input(\"Enter website URL (or 'done' to finish): \").strip()\n",
    "        if url.lower() == 'done':\n",
    "            break\n",
    "        accordion_id = input(f\"Enter accordion ID for {url}: \").strip()\n",
    "        websites.append({\"url\": url, \"accordion_id\": accordion_id})\n",
    "        logger.info(f\"Added {url} with accordion ID '{accordion_id}'\")\n",
    "    logger.info(f\"Collected {len(websites)} websites from user input\")\n",
    "    return websites\n",
    "\n",
    "def get_websites_from_file(file_path=\"websites.json\"):\n",
    "    \"\"\"Read websites and accordion IDs from a JSON file.\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.exists():\n",
    "        logger.warning(f\"File '{file_path}' not found. Creating a sample file.\")\n",
    "        sample_data = [\n",
    "            {\"url\": \"https://www.irishimmigration.ie/coming-to-visit-ireland/frequently-asked-questions/\", \"accordion_id\": \"accordion-14617-1\"}\n",
    "        ]\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sample_data, f, indent=4)\n",
    "        logger.info(f\"Created sample file '{file_path}' with 1 entry\")\n",
    "        return sample_data\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        logger.info(f\"Loaded {len(data)} websites from '{file_path}'\")\n",
    "        return data\n",
    "\n",
    "def scrape_and_save_all_faqs(output_file=\"combined_faqs.csv\", use_file=True, websites_file=\"websites.json\"):\n",
    "    \"\"\"Scrape FAQs from multiple websites and save to a single CSV.\"\"\"\n",
    "    logger.info(\"Starting FAQ scraping process\")\n",
    "    \n",
    "    # Get website list\n",
    "    if use_file:\n",
    "        websites = get_websites_from_file(websites_file)\n",
    "    else:\n",
    "        websites = get_websites_from_user()\n",
    "    \n",
    "    if not websites:\n",
    "        logger.error(\"No websites provided. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    all_faq_data = []\n",
    "    total_faqs = 0\n",
    "    \n",
    "    # Scrape each website\n",
    "    for site in websites:\n",
    "        url = site.get(\"url\")\n",
    "        accordion_id = site.get(\"accordion_id\")\n",
    "        if not url or not accordion_id:\n",
    "            logger.warning(f\"Skipping invalid entry: {site}\")\n",
    "            continue\n",
    "        \n",
    "        faq_data = scrape_faq_from_website(url, accordion_id)\n",
    "        all_faq_data.extend(faq_data)\n",
    "        total_faqs += len(faq_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    if all_faq_data:\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = [\"Website\", \"Question\", \"Answer\"]\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_faq_data)\n",
    "        logger.info(f\"Combined FAQs saved to '{output_file}'. Total FAQs: {total_faqs}\")\n",
    "    else:\n",
    "        logger.warning(\"No FAQs found to save.\")\n",
    "    \n",
    "    logger.info(\"Scraping process completed\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    # Option 1: Use a file (set use_file=True)\n",
    "    scrape_and_save_all_faqs(use_file=True, websites_file=\"websites.json\")\n",
    "    \n",
    "    # Option 2: Prompt user interactively (uncomment to use)\n",
    "    # scrape_and_save_all_faqs(use_file=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
